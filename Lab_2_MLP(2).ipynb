{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Overview\n",
    "\n",
    "## Introduction\n",
    "- Assume a neural network composed of three layers of neurons: input layer, hidden layer and output layer. We wish to train this network so when it is presented with a specific input it will have a certain output.\n",
    "- For this lab we are going to work with the [MNIST database of handwritten digits](http://yann.lecun.com/exdb/mnist) and train our network to recognise handwritten 0-9 digits. The MNIST database contains 60000 images which can used to train the network and 10000 more images for testing purposes. Each image is an  matrix representing a digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nn.png\" alt=\"neural-net\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network\n",
    "\n",
    "### Network initialisation\n",
    "\n",
    "\n",
    "1. Generate a set of weights between the input and the hidden layer. The input layer should have 784 neurons, one for each pixel of the image.\n",
    "\n",
    "2. Generate a set of weights between the hidden and the output layer. The output layer should have 10 neurons, one for each digit.\n",
    "\n",
    "3. Generate two sets of bias, one for the hidden layer and one for the output layer. Bias is set to 0 to start with \n",
    "\n",
    "\n",
    "### Feedforward\n",
    "\n",
    "\n",
    "1. Feed an image $\\vec{x_\\mu}$ to the network,\n",
    "   \n",
    "\n",
    "2. Compute the input to each of the neurons of the hidden layer, $h^{(1)}_i = \\sum_{i=1}^{784}w^{(1)}_{ij} x^{(0)}_j + b^{(1)}_i$ and their outputs with the use of the [sigmoid function](http://mathworld.wolfram.com/SigmoidFunction.html),$x^{(1)}_i = f(h^{(1)}_i) = \\frac{1}{(1+e^{-h^{(1)}_i})}$.\n",
    "\n",
    "3. Repeat for the output layer, $h^{(2)}_i = \\sum_{i=1}^{784}w^{(2)}_{ij} x^{(1)}_j + b^{(2)}_i$ and their outputs, $x^{(2)}_i = f(h^{(2)}_i)$\n",
    "\n",
    "<img src=\"nn1.png\" alt=\"neural-net\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "### Error\n",
    "\n",
    "1. Calculate the error in the output of the neural network. If the input is an image of a zero digit it is expected that the first neuron of the input layer to be fully active, i.e has an output of 1, while the other neurons have outputs of 0. Thus the target output would be $t = [1,0,0,\\dots,0]^T$ and, given that the output of the neural network is some $output$, the error is given by the formula $$E(\\mu) = \\sum_{i=1}^{n}\\frac{1}{2}(t_{\\mu i}-x^{(2)}_{\\mu i})^2,$$ where $n$ is the number of neurons in the output layer of the network and $\\mu$ is sample index. The total error per sample is then given by $$E/N = \\sum_{\\mu=1}^N \\sum_{i=1}^{n}\\frac{1}{2N}(t_{\\mu i}-x^{(2)}_{\\mu i})^2.$$\n",
    "\n",
    "<img src=\"nn2.png\" alt=\"neural-net\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "### Backpropagation\n",
    "Here we will consider explicitly the logistic sigmoid activation function so we can use the result that the derivative of the logistic sigmoid is\n",
    "$$ f'(h) = \\frac{\\partial f(h) }{\\partial h } = f(h)(1-f(h)) $$ directly in the update rules.\n",
    "\n",
    "1. Apply the delta rule between the output and the hidden layers. For the weight $w_{ij}$ we have \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{\\textrm{ij}} }=\\frac{\\partial E}{\\partial x^{(2)}_i }\\;\\frac{\\partial x^{(2)}_i }{\\partial h^{(2)}_i } \\; \\frac{\\partial h^{(2)}_i }{\\partial w_{\\textrm{ij}} } \\; =-\\left( t_i -x^{(2)}_i \\right)\\; f'\\left(h^{(2)}_i \\right) \\; x^{(1)}_j,$$ \n",
    "\n",
    "and we can define  $\\delta^{(2)}_i = \\left(t_i - x^{(2)}_i \\right) f'\\left(h^{(2)}_i \\right) $.\n",
    "\n",
    "2. Update the weights between the hidden and the output layers.  For the weight $w_{ij}$ we have the update after a single input $${\\Delta w}_{\\textrm{ij}} = \\eta \\delta^{(2)}_i x^{(1)}_j.$$\n",
    "\n",
    "3. Repeat steps 1 and 2 between the hidden and the input layes and update the weights between them. Here for the weight $w_{ij}$ we have \n",
    "$$\\frac{\\partial E}{\\partial w_{\\textrm{ij}} }=\\frac{\\partial E}{\\partial x^{(1)}_j }\\;\\frac{\\partial x^{(1)}_j }{\\partial h^{(1)}_j }\\;\\frac{\\partial h^{(1)}_j }{\\partial w_{\\textrm{ij}} }\n",
    "=x^{(0)}_j x^{(1)}_i \\;\\left(1-x^{(1)}_i \\right) \\sum_k \\left(w^{(2)}_{\\textrm{ki}} \\delta^{(2)}_k \\right),$$ where the terms $w_{\\textrm{ij}}^{\\left(2\\right)}, \\delta_j^{\\left(2\\right)}$ are the weights and the delta function of the previous layer. So now we can defined the backpropagated local gradient for the hidden layer as\n",
    "$$ \\delta_i^{(1)} = x^{(1)}_i \\;\\left(1-x^{(1)}_i \\right) \\sum_j \\delta^{(2)}_j w^{(2)}_{ji}. $$\n",
    "\n",
    "4. Update the weights between the hidden and the input layers. For the weight $w_{ij}$ we have $${\\Delta w}_{\\textrm{ij}} =\\eta \\delta^{(1)}_i x^{(0)}_j.$$\n",
    "\n",
    "<img src=\"nn3.png\" alt=\"neural-net\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The code provided first loads the MNIST database and generates a neural network using the parameters provided. Then it trains the network and tests it using the testing set of the MNIST database.\n",
    "\n",
    "1. Test the network using the test dataset, then compute and comment on it's accuracy.\n",
    "\n",
    "2. The weights are generating from a uniformly distribution in the interval (0,1) and are then normalized using the sum equals to 1 normalization, i.e the summation of the inputs should be equal to 1. Why is this normalization important? Another initialisation method is the [Xavier initialization](http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf?hc_location=ufi), which is more commonly used in deep neural networks of more than one hidden layer. In this initialisation the weights are initialised using normally distributed random numbers which are then scaled by sqrt(1/n_cols). Change your initialisation to this and comment on how this changes the training.\n",
    "\n",
    "3. A bias is included in the calculation of the activation but is set to zero and is not trained. Derive the learning rule and code the training of the bias alongside the weights. It may help to think of the bias as an extra weight where the input value is fixed to 1.\n",
    "\n",
    "4. Initially the network updates the weights once per epoch. Adapt the code to split the data into mini-batches, where the weights are updated after each mini-batch. How does this affect the training of the network? Be careful with the learning rate as you decrease the batch_size.\n",
    "\n",
    "5. Tune the network by finding appropriate values for the parameters: number of epochs, number of neurons of the hidden layer and learning rate.\n",
    "\n",
    "6. Upgrade the network by adding one more hidden layer, derive the appropriate formulas for the feedforward and the backpropagation and program them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic sigmoid function and it's derivative\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "def grad_sigmoid(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset:\n",
    "#     The MNIST dataset contains 60,000 training samples.\n",
    "#     To start with we will limit this 1000 to speed up development\n",
    "#     When you are happy that your code is correct try training on more samples \n",
    "mnist = loadmat('MNIST.mat')\n",
    "\n",
    "# Read the first 1000 from the train set\n",
    "x_train = mnist['x_train'][:1000]\n",
    "# Read the first 1000 train labels\n",
    "trainlabels = mnist['trainlabels'][:1000]\n",
    "\n",
    "# Read the test set\n",
    "x_test = mnist['x_test']\n",
    "# Read the test labels\n",
    "testlabels = mnist['testlabels']\n",
    "\n",
    "# Show the shape of each of these arrays\n",
    "# Our convention is for the first dimension to be the number of samples\n",
    "print(x_train.shape)\n",
    "print(trainlabels.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(testlabels.shape)\n",
    "\n",
    "# Extract the size of the dataset and the size of each input\n",
    "n_samples, img_size = x_train.shape\n",
    "\n",
    "# The MNIST contains the digits 0 to 9 so we will set the number of labels as 10\n",
    "nlabels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of image, take the transpose to see it in the right orientation\n",
    "example_image=np.reshape(x_train[0], (28,28))\n",
    "plt.imshow(example_image.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1-hot encoded output for the desired output\n",
    "y_train = np.zeros((trainlabels.shape[0], nlabels))\n",
    "y_test  = np.zeros((testlabels.shape[0], nlabels))\n",
    "\n",
    "# For each input, set only the index that corresponds to the \n",
    "# correct label to 1, the rest stay equal to 0\n",
    "for i in range(0,trainlabels.shape[0]):   \n",
    "    y_train[i, trainlabels[i].astype(int)]=1\n",
    "    \n",
    "for i in range(0,testlabels.shape[0]):    \n",
    "    y_test[i, testlabels[i].astype(int)]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of epochs is a hyperparameter that defines the number times that the learning algorithm \n",
    "# will work through the entire training dataset.\n",
    "n_epoch = 50\n",
    "\n",
    "\n",
    "# The batch size is a hyperparameter that defines the number of samples to work through before \n",
    "# updating the internal model parameters. \n",
    "# ref: https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n",
    "batch_size = n_samples\n",
    "n_batches = 1\n",
    "\n",
    "# define the size of each of the layers in the network\n",
    "n_input_layer  = img_size\n",
    "n_hidden_layer = 50\n",
    "n_output_layer = nlabels\n",
    "\n",
    "# eta is the learning rate\n",
    "eta = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a simple network\n",
    "# For W1 and W2 columns are the input and the rows are the output.\n",
    "# W1: Number of columns (input) needs to be equal to the number of features \n",
    "#     of the  MNIST digits, thus p. Number of rows (output) should be equal \n",
    "#     to the number of neurons of the hidden layer thus n_hidden_layer.\n",
    "# W2: Number of columns (input) needs to be equal to the number of neurons \n",
    "#     of the hidden layer. Number of rows (output) should be equal to the \n",
    "#     number of digits we wish to find (classification).\n",
    "\n",
    "W1 = np.random.uniform(0, 1, size=(n_hidden_layer, n_input_layer))\n",
    "W2 = np.random.uniform(0, 1, size=(n_output_layer, n_hidden_layer))\n",
    "\n",
    "\n",
    "# The following normalises the random weights so that the sum of each row =1\n",
    "# np.newaxis inserts a new dimension to the array ready for broadcasting\n",
    "W1 = np.divide(W1, np.sum(W1, axis=1)[:,np.newaxis])\n",
    "W2 = np.divide(W2, np.sum(W2, axis=1)[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the biases\n",
    "bias_W1 = np.zeros((n_hidden_layer,))\n",
    "bias_W2 = np.zeros((n_output_layer,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the network inputs and average error per epoch\n",
    "errors = np.zeros((n_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the network\n",
    "\n",
    "# Loop over the whole dataset multiple times\n",
    "for i in range(0,n_epoch):\n",
    "    \n",
    "    # Initialise the gradients for each batch to zero\n",
    "    dW1 = np.zeros(W1.shape)\n",
    "    dW2 = np.zeros(W2.shape)\n",
    "    \n",
    "    dbias_W1 = np.zeros(bias_W1.shape)\n",
    "    dbias_W2 = np.zeros(bias_W2.shape)\n",
    "    \n",
    "    # We will shuffle the order of the samples each epoch\n",
    "    shuffled_idxs = np.random.permutation(n_samples)\n",
    "    \n",
    "    # Loop over all the samples in the batch\n",
    "    for j in range(0,batch_size):\n",
    "        \n",
    "        # Input (random element from the dataset)\n",
    "        idx = shuffled_idxs[j]\n",
    "        x0 = x_train[idx]\n",
    "        \n",
    "        # Neural activation: input layer -> hidden layer\n",
    "        h1 = np.matmul(W1, x0)+bias_W1\n",
    "        \n",
    "        # Apply the sigmoid function\n",
    "        x1 = sigmoid(h1)\n",
    "    \n",
    "        # Neural activation: hidden layer -> output layer\n",
    "        h2 = np.matmul(W2, x1)+bias_W2\n",
    "        \n",
    "        # Apply the sigmoid function\n",
    "        x2 = sigmoid(h2)\n",
    "        \n",
    "        # Form the desired output, the correct neuron should have 1 the rest 0\n",
    "        desired_output = y_train[idx]\n",
    "        \n",
    "        # Compute the error signal\n",
    "        e_n = desired_output - x2\n",
    "        \n",
    "        # Backpropagation: output layer -> hidden layer\n",
    "        delta2 = grad_sigmoid(h2) * e_n\n",
    "        dW2 += np.outer(delta2, x1)\n",
    "        \n",
    "        # Backpropagation: hidden layer -> input layer\n",
    "        delta1 = grad_sigmoid(h1) * np.dot(W2.T, delta2)\n",
    "        dW1 += np.outer(delta1, x0)\n",
    "                \n",
    "        # Store the error per epoch\n",
    "        errors[i] = errors[i] + 0.5*np.sum(np.square(e_n))/n_samples\n",
    "    \n",
    "    # After each batch update the weights using accumulated gradients\n",
    "    W2 += eta*dW2/batch_size\n",
    "    W1 += eta*dW1/batch_size\n",
    "    \n",
    "    print( \"Epoch \", i+1, \": error = \", errors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the performance\n",
    "plt.plot(errors)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Average error per epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use the test set to compute the network's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
