{"cells":[{"cell_type":"markdown","metadata":{"id":"kijknHbwgfE7"},"source":["# Lab 4: Competitive learning on the MNIST database\n","\n","## Learning Outcomes\n","- Understand how competive learning works.\n","- Ability to develop and tune a simple neural network using competitive learning."]},{"cell_type":"markdown","metadata":{"id":"EBrLJN1_gfE_"},"source":["## Lab Overview\n","\n","### Competitive learning\n","- The aim of this lab is to implement the standard competitive learning algorithm on a one layer network and use it to cluster the hand written digits (MNIST) data set.\n","- Clustering: given a set of datapoints we want to group them based on their similarity. Each one of these groups is called cluster.\n","- Using competitive learning, the output neurons compete amongst themselves to be activated (fire).\n","- In contrast with the Hebbian learning, here only one (output) neuron (or group of neurons) can be active at a time.\n","- The output neuron that wins the competition is is often called the winner-take-all neuron.\n","\n","\n","<img src=\"competitive.png\" width=\"400\">\n","\n","\n","### Main steps for competitive learning implementation:\n","1. Generate a set of output neurons with random weights.\n","2. Choose a random input pattern and calculate the activation (total input) of each of the output neurons using:\n","$$ h_i = \\sum_j W_{ij} x^{\\mu}_j $$\n","3. Detect the winner neuron and update only its weights (the weights of the other neurons are not updated):\n","$$ \\Delta W_{kj} = \\eta ( x^{\\mu}_j - W_{kj}) $$\n","    where $k$ is the index of winning neuron, $\\eta$ is the learning rate and $x^{\\mu}_j$ is the input vector.\n","4. Repeat from step 2 until the weights are no longer changing, or change less than a set threshold, or a set maximum number of iterations has been reached.\n","\n","### Reconstruction Error and Exponential Moving Average\n","A measure of how well the network is clustering the datapoints can be given using the *Reconstruction Error*. For an individual datapoint the reconstruction error is the squared difference between the datapoint and the weight vector of the cluster it belongs to. Remember, in competitive learning the data point belongs to the cluster for which the neuron activation is the largest. If $k$ is the winning neuron then this individual error for pattern $\\mu$ is\n","$$ E^{\\mu} = \\sum_j ( x^{\\mu}_j - W_{kj})^2 $$ \n","or in vector notation we can write this as (assuming both are column vectors): \n","$$ E^{\\mu} = (\\mathbf{x^{\\mu}} - \\mathbf{W}_k)^T (\\mathbf{x^{\\mu}} - \\mathbf{W}_k) $$.\n","\n","The total reconstruction error can be calculated by summing over the whole dataset (as in the lectures):\n","$$ E = \\sum_\\mu E^{\\mu} = \\sum_\\mu \\sum_j ( x^{\\mu}_j - W_{kj})^2, $$\n","or alternatively we will track an exponential moving average of the individual error. This applys a smoothing to individual errors and allows us to monitor how the error is dropping. The average is calculated using\n","$$ \\bar{E}_t = (1-\\alpha) \\bar{E}_{t-1}  + \\alpha E^{\\mu},$$\n","where $\\bar{E}_t$ is the moving average at iteration $t$ and $\\alpha$ is the smoothing rate. For the first iteration $t=0$, we need to consider the boundary condition, since we have no value $\\bar{E}_{t-1}$ we have a special case and can start the moving average using\n","$$ \\bar{E}_{0} = E^{\\mu}, $$\n","i.e we set the moving average to first measurement of the error."]},{"cell_type":"markdown","metadata":{"id":"GxRccctLgfFA"},"source":["### Correlation Matrix of the Prototypes\n","\n","As your network finds clusters and the prototypes (weights) come to represent these clusters, you will notice that there are similarities between different prototypes. We can analyse these similarities by calculating the correlation between two prototypes, similar to how in class we looked at the correlation between inputs. In case of the prototypes we are looking for similarities over the input dimension (i.e 2nd dimension of the weight matrix, so the elements of the correlation matrix are\n","$$ C_{ij} = \\sum_k W_{ik} W_{jk}, $$\n","where $C_{ij}$ is the correlation between prototypes (weights) $i$ and $j$. This will be symmetric (consider swapping $i$ and $j$ in the above equation) and the diagonal will represent the squared magnitude ($|\\mathbf{W}_i|^2$) of the prototypes. If the weights are no longer normalised you should consider normalising them before calculating this so only similarity is measured and not the difference in magnitudes."]},{"cell_type":"markdown","metadata":{"id":"LHgJEmKggfFA"},"source":["\n","### Exercise\n","Using the skeleton code below implement the competitive learning algorithm as detailed above. \n","Please choose the number of output units such that you can capture all possible clusters, and tune the network such that it will learn quickly and result in as few dead units as possible. One suggestion is to add noise to the decision neurons, in combination with an appropriate decaying learning rate, but you are free to apply other techniques. In this example you are able to visualise both data and prototypes, and can easily locate the dead units. The vectors provided are 28x28 images and you can reshape them to see the digits. \n","\n","You will need to do the following :\n","1. Create a figure showing the average error as a function of time. When has your network has sufficiently learned from the data? Such a curve may be more informative on semi-log or log-log axes.\n","2. Propose a method for detecting dead units, without using the visualisation of the prototypes.\n","3. Create a figure of the prototypes (weights). How many prototypes did your network find?\n","4. Calculate the correlation matrix of the prototypes. How can you use this information to find similarities between the prototypes?"]},{"cell_type":"markdown","metadata":{"id":"Q-RxNlFngfFB"},"source":["### Some optimisation and tuning methods\n","You may find that your training results in a lot of dead units. To reduce these you may need to consider the following optimisation of tuning methods: \n","1. Normalised input or initial weights.\n","2. Noise addition on the weights.\n","3. Decaying learning rate.\n","4. Leaky learning: update the weights of the losers as well as winners but with a much smaller learning rate.\n","5. Update the winners and neighbouring losers."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"TaPI9PxtgfFB","executionInfo":{"status":"ok","timestamp":1652575741630,"user_tz":-60,"elapsed":738,"user":{"displayName":"Hao-Hsuan Teng","userId":"13542298947368054088"}}},"outputs":[],"source":["import numpy as np\n","import numpy.matlib\n","import math\n","import matplotlib\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import normalize #normalize\n","from numpy import random #random noise"]},{"cell_type":"markdown","metadata":{"id":"b7-WRdfLgfFC"},"source":["#### Load the data set"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"6gNYIxfygfFD","colab":{"base_uri":"https://localhost:8080/","height":488},"executionInfo":{"status":"error","timestamp":1652575742115,"user_tz":-60,"elapsed":488,"user":{"displayName":"Hao-Hsuan Teng","userId":"13542298947368054088"}},"outputId":"3c380e83-5afd-4ae0-9f7f-defe22f8131d"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'emnist_capletters_AtoJ.mat'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-df03d27cc469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'emnist_capletters_AtoJ.mat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze_me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_images'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \"\"\"\n\u001b[1;32m    215\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reader needs file name or open file-like object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'emnist_capletters_AtoJ.mat'"]}],"source":["# Load the training dataset\n","import scipy.io as spio\n","mat = spio.loadmat('emnist_capletters_AtoJ.mat', squeeze_me=True)\n","\n","train = mat['train_images'] # data\n","\n","for data in train:                  #normalised\n","  data /= np.sqrt(np.dot(data, data))\n","\n","[n_samples, n_pixels]  = train.shape                   \n","\n","print(\"n_samples = \", n_samples)    # n_samples = 5,000 (images)\n","print(\"n_pixels  = \", n_pixels)     # n_pixels = 784 = 28x28 pixels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iACo3nSIgfFE","executionInfo":{"status":"aborted","timestamp":1652575742112,"user_tz":-60,"elapsed":232,"user":{"displayName":"Hao-Hsuan Teng","userId":"13542298947368054088"}}},"outputs":[],"source":["# Plot one of the mnist images\n","plt.imshow(train[0].reshape((28, 28), order='F'))\n","cbar = plt.colorbar()\n","cbar.set_label('Pixel Intensity')\n","plt.show()\n","\n","print('Magnitude of image vector |x| = ', np.sqrt( np.dot(train[0], train[0])))"]},{"cell_type":"markdown","metadata":{"id":"-pP-PkAvgfFE"},"source":["#### Parameters and variables "]},{"cell_type":"markdown","metadata":{"id":"vQJFo-wJgfFF"},"source":["####  Implementation, training and output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXXjOVX0gfFF","executionInfo":{"status":"aborted","timestamp":1652575742113,"user_tz":-60,"elapsed":233,"user":{"displayName":"Hao-Hsuan Teng","userId":"13542298947368054088"}}},"outputs":[],"source":["avg_deadunit = 0\n","\n","#fig = plt.figure()\n","#ax = plt.subplot(111)\n","n_output_list = [10,15,20]\n","\n","#for n_output_num in n_output_list:       \n","for i in range(10):\n","    # Parameters\n","    eta      = 0.05     # learning rate \n","    winit    = 1     # parameter controlling magnitude of initial weights #original 1\n","    tmax     = 10000  # number of iterations #original 40\n","    n_output = 10      # number of output neurons   #change to n_output_num to \n","    alpha    = 0.01   # Moving average decay factor\n","\n","    # Weight matrix (rows = output neurons, cols = input neurons)\n","    #W = winit * np.random.rand(n_output, n_pixels)  #Initialise weight with random value\n","\n","    W = np.zeros([n_output, n_pixels])     #Initialise weight with sample\n","\n","    for i in range(n_output):               #Select samples from train data\n","      W[i] = winit * train[np.random.default_rng().integers(n_samples)]\n","\n","    for i in range(n_output):\n","        W[i] /= np.sqrt(np.dot(W[i], W[i]))\n","\n","    deadUnit = 0\n","\n","    rng = np.random.default_rng()\n","\n","    # Count how many times each output neuron wins\n","    counter = np.zeros(n_output, dtype=int)\n","\n","    update_counter = np.zeros(n_output, dtype=int)  #add bias counter\n","\n","    winning_neuron = np.zeros(n_samples, dtype=int)\n","    moving_avg_error = np.zeros(tmax)\n","    for t in range(tmax):\n","      i = rng.integers(n_samples)\n","      x = train[i]\n","\n","      h = np.matmul(W,x) + random.rand(n_output)*1e-07 #noise\n","      h = h + h*(update_counter*4e-07)   #add bias\n","\n","      k = np.argmax(h)\n","\n","      winning_neuron[i] = k \n","      counter[k] += 1\n","      #count update  #add bias\n","      update_counter += 1\n","      update_counter[k] = 0\n","\n","\n","      error = np.sum(np.square(x-W[k]))\n","      if t == 0:\n","          moving_avg_error[t] = error\n","      else:\n","          moving_avg_error[t] = (1-alpha)*moving_avg_error[t-1] + alpha*error\n","          \n","      dW = eta*(x-W[k])\n","      \n","      W[k] = W[k] + dW\n","\n","      W[k] /= np.sqrt(np.dot(W[k], W[k]))   #normalise again after sample\n","\n","      #Leaky learning\n","      for i in range(W.shape[0]):\n","        if i != k:\n","          dW = (eta/1000)*(x-W[i]) \n","          W[i] = W[i] + dW\n","          W[i] /= np.sqrt(np.dot(W[i], W[i]))   #Should be uncomment if initialise weight with train sample is enable\n","      \n","      #Update the winners and neighbouring losers\n","      for i in range(2,6):\n","        k = np.argsort(h)[-i]\n","        dW = (eta/10)*(6-i)/10 *(x-W[k])\n","        W[k] = W[k] + dW\n","        W[k] /= np.sqrt(np.dot(W[k], W[k]))   #normalise again after sample, should be uncomment if initialise weight with train sample is enable \n","  \n","    deadUnit = len(np.where(counter < np.max(counter)/5)[0]) #Calculate dead unit\n","\n","    avg_deadunit = avg_deadunit+deadUnit  \n","    #ax.semilogy(moving_avg_error, label=\"n_output:\"+str(n_output_num))\n","    #ax.legend()\n","\n","print(\"Average dead unit:\" + str(avg_deadunit/10))\n","#plt.savefig('moving_avg_error.png', bbox_inches='tight')\n","#plt.show()\n","#plt.clf()"]},{"cell_type":"code","source":["def plotPrototypes(W):      #plot weight prototypes\n","  plt.figure()\n","  \n","  fig, ax = plt.subplots(5, 2, figsize=(5,15))\n","  ax = ax.flatten()\n","  for i, w in enumerate(W):\n","    im = ax[i].imshow(w.reshape((28, 28), order='F'))\n","\n","  fig.subplots_adjust(right=0.8)\n","  fig.colorbar(im, fig.add_axes([0.9, 0.15, 0.06, 0.7]))\n","\n","  \n","\n","  plt.savefig('prototypes.png', bbox_inches='tight')\n","  plt.show()\n","\n","\n","plotPrototypes(W)"],"metadata":{"id":"PhZqjhxQJlo-","executionInfo":{"status":"aborted","timestamp":1652575742113,"user_tz":-60,"elapsed":233,"user":{"displayName":"Hao-Hsuan Teng","userId":"13542298947368054088"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.DataFrame(W.transpose())\n","\n","corrMatrix = df.corr()\n","\n","plt.matshow(corrMatrix)\n","\n","cbar = plt.colorbar()\n","\n","plt.savefig('corrMatrix.png')\n","\n","plt.show()"],"metadata":{"id":"a1Cr0lrns5I4","executionInfo":{"status":"aborted","timestamp":1652575742114,"user_tz":-60,"elapsed":234,"user":{"displayName":"Hao-Hsuan Teng","userId":"13542298947368054088"}}},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"colab":{"name":"Lab4.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}